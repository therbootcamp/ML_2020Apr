<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title>Optimization</title>

<script src="Optimization_practical_files/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="Optimization_practical_files/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="Optimization_practical_files/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="Optimization_practical_files/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="Optimization_practical_files/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="Optimization_practical_files/navigation-1.1/tabsets.js"></script>
<link href="Optimization_practical_files/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="Optimization_practical_files/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>

<link rel="stylesheet" href="practical.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>



<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Optimization</h1>
<h4 class="author"><table style='table-layout:fixed;width:100%;border:0;padding:0;margin:0'>
<col width='10%'>
<col width='10%'>
<tr style="border:none">
<td style="display:block;width:100%;text-align:left;vertical-align:bottom;padding:0;margin:0;border:none" nowrap>
<font style='font-style:normal'>Machine Learning with R</font><br> <a href='https://therbootcamp.github.io/ML_2019Oct/'> <i class='fas fa-clock' style='font-size:.9em;' ></i> </a> <a href='https://therbootcamp.github.io'> <i class='fas fa-home' style='font-size:.9em;'></i> </a> <a href='mailto:therbootcamp@gmail.com'> <i class='fas fa-envelope' style='font-size: .9em;'></i> </a> <a href='https://www.linkedin.com/company/basel-r-bootcamp/'> <i class='fab fa-linkedin' style='font-size: .9em;'></i> </a> <a href='https://therbootcamp.github.io'> <font style='font-style:normal'>Basel R Bootcamp</font> </a>
</td>
<td style="width:100%;vertical-align:bottom;text-align:right;padding:0;margin:0;border:none">
<img src='https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/by-sa.png' style='height:15px;width:80px'/>
</td>
</tr>
</table></h4>

</div>


<p align="center">
<img width="100%" src="image/rexthor.png" margin=0><br> <font style="font-size:10px">from <a href="https://xkcd.com/1725/">xkcd.com</a></font>
</p>
<div id="section" class="section level1 tabset">
<h1></h1>
<div id="overview" class="section level2">
<h2>Overview</h2>
<p>By the end of this practical you will know how to:</p>
<ol style="list-style-type: decimal">
<li>Use cross-validation to select optimal model tuning parameters for decision trees and random forests.</li>
<li>Compare ‘standard’ regression with lasso and ridge penalised regression.</li>
<li>Use cross-validation to estimate future test accuracy.</li>
</ol>
</div>
<div id="tasks" class="section level2">
<h2>Tasks</h2>
<div id="baseball-player-salaries" class="section level3">
<h3>Baseball player salaries</h3>
<p>In this practical, we will predict the <code>Salary</code> of baseball players from the <code>hitters_train</code> and <code>hitters_test</code> datasets.</p>
</div>
<div id="a---setup" class="section level3">
<h3>A - Setup</h3>
<ol style="list-style-type: decimal">
<li><p>Open your <code>BaselRBootcamp</code> R project. It should already have the folders <code>1_Data</code> and <code>2_Code</code>. Make sure that the data file(s) listed in the <code>Datasets</code> section are in your <code>1_Data</code> folder</p></li>
<li><p>Open a new R script. At the top of the script, using comments, write your name and the date. Save it as a new file called <code>Optimization_practical.R</code> in the <code>2_Code</code> folder.</p></li>
<li><p>Using <code>library()</code> load the set of packages for this practical listed in the packages section above.</p></li>
</ol>
<pre class="r"><code># Load packages necessary for this script
library(tidyverse)
library(caret)
library(party)
library(partykit)</code></pre>
<ol start="4" style="list-style-type: decimal">
<li>Run the code below to load each of the datasets listed in the <code>Datasets</code> section as new objects.</li>
</ol>
<pre class="r"><code># hitters data
hitters_train &lt;- read_csv(file = &quot;1_Data/hitters_train.csv&quot;)
hitters_test &lt;- read_csv(file = &quot;1_Data/hitters_test.csv&quot;)</code></pre>
<ol start="5" style="list-style-type: decimal">
<li>Take a look at the first few rows of each dataframe by printing them to the console.</li>
</ol>
<pre class="r"><code># Print dataframes to the console
hitters_train
hitters_test</code></pre>
<ol start="6" style="list-style-type: decimal">
<li>Print the numbers of rows and columns of each dataset using the <code>dim()</code> function.</li>
</ol>
<pre class="r"><code># Print numbers of rows and columns
dim(XXX)
dim(XXX)</code></pre>
<pre class="r"><code>dim(hitters_train)</code></pre>
<pre><code>[1] 50 20</code></pre>
<pre class="r"><code>dim(hitters_test)</code></pre>
<pre><code>[1] 213  20</code></pre>
<ol start="7" style="list-style-type: decimal">
<li>Look at the names of the dataframes with the <code>names()</code> function.</li>
</ol>
<pre class="r"><code># Print the names of each dataframe
names(XXX)
names(XXX)</code></pre>
<pre class="r"><code>names(hitters_train)</code></pre>
<pre><code> [1] &quot;Salary&quot;    &quot;AtBat&quot;     &quot;Hits&quot;      &quot;HmRun&quot;     &quot;Runs&quot;     
 [6] &quot;RBI&quot;       &quot;Walks&quot;     &quot;Years&quot;     &quot;CAtBat&quot;    &quot;CHits&quot;    
[11] &quot;CHmRun&quot;    &quot;CRuns&quot;     &quot;CRBI&quot;      &quot;CWalks&quot;    &quot;League&quot;   
[16] &quot;Division&quot;  &quot;PutOuts&quot;   &quot;Assists&quot;   &quot;Errors&quot;    &quot;NewLeague&quot;</code></pre>
<pre class="r"><code>names(hitters_test)</code></pre>
<pre><code> [1] &quot;Salary&quot;    &quot;AtBat&quot;     &quot;Hits&quot;      &quot;HmRun&quot;     &quot;Runs&quot;     
 [6] &quot;RBI&quot;       &quot;Walks&quot;     &quot;Years&quot;     &quot;CAtBat&quot;    &quot;CHits&quot;    
[11] &quot;CHmRun&quot;    &quot;CRuns&quot;     &quot;CRBI&quot;      &quot;CWalks&quot;    &quot;League&quot;   
[16] &quot;Division&quot;  &quot;PutOuts&quot;   &quot;Assists&quot;   &quot;Errors&quot;    &quot;NewLeague&quot;</code></pre>
<ol start="8" style="list-style-type: decimal">
<li>Open each dataset in a new window using <code>View()</code>. Do they look ok?</li>
</ol>
<pre class="r"><code># Open each dataset in a window.
View(XXX)
View(XXX)</code></pre>
<ol start="9" style="list-style-type: decimal">
<li>As always, we need to convert all character columns to factors before we start. Do this by running the following code.</li>
</ol>
<pre class="r"><code># Convert all character columns to factor
hitters_train &lt;- hitters_train %&gt;%
          mutate_if(is.character, factor)

hitters_test &lt;- hitters_test %&gt;%
          mutate_if(is.character, factor)</code></pre>
</div>
<div id="b---setup-traincontrol" class="section level3">
<h3>B - Setup <code>trainControl</code></h3>
<ol style="list-style-type: decimal">
<li>Set up your training by specifying <code>ctrl_cv</code> as 10-fold cross-validation. Specifically,…</li>
</ol>
<ul>
<li>set <code>method = &quot;cv&quot;</code> to specify cross validation.</li>
<li>set <code>number = 10</code> to specify 10 folds.</li>
</ul>
<pre class="r"><code># Use 10-fold cross validation
ctrl_cv &lt;- trainControl(method = &quot;XX&quot;, 
                        number = XX) </code></pre>
<pre class="r"><code># Use 10-fold cross validation
ctrl_cv &lt;- trainControl(method = &quot;cv&quot;, 
                        number = 10) </code></pre>
</div>
<div id="c---regression-standard" class="section level3">
<h3>C - Regression (standard)</h3>
<ol style="list-style-type: decimal">
<li>Fit a (standard) regression model predicting <code>Salary</code> as a function of all features. Specifically,…</li>
</ol>
<ul>
<li>set the formula to <code>Salary ~ .</code>.</li>
<li>set the data to <code>hitters_train</code>.</li>
<li>set the method to <code>&quot;glm&quot;</code> for regular regression.</li>
<li>set the train control argument to <code>ctrl_cv</code>.</li>
</ul>
<pre class="r"><code># Normal Regression --------------------------
salary_glm &lt;- train(form = XX ~ .,
                    data = XX,
                    method = &quot;XX&quot;,
                    trControl = XX)</code></pre>
<pre class="r"><code># Normal Regression --------------------------
salary_glm &lt;- train(form = Salary ~ .,
                   data = hitters_train,
                   method = &quot;glm&quot;,
                   trControl = ctrl_cv)</code></pre>
<ol start="2" style="list-style-type: decimal">
<li>Print your <code>salary_glm</code>. What do you see?</li>
</ol>
<pre class="r"><code>salary_glm</code></pre>
<pre><code>Generalized Linear Model 

50 samples
19 predictors

No pre-processing
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 43, 45, 45, 46, 45, 46, ... 
Resampling results:

  RMSE  Rsquared  MAE
  575   0.377     490</code></pre>
<ol start="3" style="list-style-type: decimal">
<li>Try plotting your <code>salary_glm</code> object. What happens? What does this error mean?</li>
</ol>
<pre class="r"><code># I get the following error:

# Error in plot.train(salary_glm) : 
#   There are no tuning parameters for this model.


# The problem is that method = &quot;glm&quot; has no tuning parameters so there is nothing to plot!</code></pre>
<ol start="4" style="list-style-type: decimal">
<li>Print your final model object with <code>salary_glm$finalModel</code>.</li>
</ol>
<pre class="r"><code># Print final regression model
salary_glm$finalModel</code></pre>
<pre><code>
Call:  NULL

Coefficients:
(Intercept)        AtBat         Hits        HmRun         Runs  
   664.0169      -1.8086      12.7263      14.0206     -10.7594  
        RBI        Walks        Years       CAtBat        CHits  
    -5.1859      -8.6702     -62.3666       0.4883      -4.9553  
     CHmRun        CRuns         CRBI       CWalks      LeagueN  
    -8.3973       5.7164       4.3061       0.3754     144.0316  
  DivisionW      PutOuts      Assists       Errors   NewLeagueN  
   -75.0233      -0.0843       0.5934      -9.7698     223.6980  

Degrees of Freedom: 49 Total (i.e. Null);  30 Residual
Null Deviance:      12600000 
Residual Deviance: 5110000  AIC: 761</code></pre>
<ol start="5" style="list-style-type: decimal">
<li>Print your final regression model coefficients with <code>coef()</code>.</li>
</ol>
<pre class="r"><code># Print glm coefficients
coef(salary_glm$finalModel)</code></pre>
<pre><code>(Intercept)       AtBat        Hits       HmRun        Runs         RBI 
   664.0169     -1.8086     12.7263     14.0206    -10.7594     -5.1859 
      Walks       Years      CAtBat       CHits      CHmRun       CRuns 
    -8.6702    -62.3666      0.4883     -4.9553     -8.3973      5.7164 
       CRBI      CWalks     LeagueN   DivisionW     PutOuts     Assists 
     4.3061      0.3754    144.0316    -75.0233     -0.0843      0.5934 
     Errors  NewLeagueN 
    -9.7698    223.6980 </code></pre>
</div>
<div id="d---ridge-regression" class="section level3">
<h3>D - Ridge Regression</h3>
<p>It’s time to fit an optimized regression model with a Ridge penalty!</p>
<ol style="list-style-type: decimal">
<li>Before we can fit a ridge regression model, we need to specify which values of the lambda penalty parameter we want to try. Using the code below, create a vector called <code>lambda_vec</code> which contains 100 values spanning a wide range, from very close to 0 to 1,000.</li>
</ol>
<pre class="r"><code># Vector of lambda values to try
lambda_vec &lt;- 10 ^ (seq(-4, 4, length = 100))</code></pre>
<ol start="2" style="list-style-type: decimal">
<li>Fit a ridge regression model predicting <code>Salary</code> as a function of all features. Specifically,…</li>
</ol>
<ul>
<li>set the formula to <code>Salary ~ .</code>.</li>
<li>set the data to <code>hitters_train</code>.</li>
<li>set the method to <code>&quot;glmnet&quot;</code> for regularized regression.</li>
<li>set the train control argument to <code>ctrl_cv</code>.</li>
<li>set the <code>preProcess</code> argument to <code>c(&quot;center&quot;, &quot;scale&quot;)</code> to make sure the variables are standarised when estimating the beta weights (this is good practice for ridge regression as otherwise the different scales of the variables impact the betas and thus the punishment would also depend on the scale).</li>
<li>set the tuneGrid argument such that alpha is 0 (for ridge regression), and with all lambda values you specified in <code>lambda_vec</code> (we’ve done this for you below).</li>
</ul>
<pre class="r"><code># Ridge Regression --------------------------
salary_ridge &lt;- train(form = XX ~ .,
                      data = XX,
                      method = &quot;XX&quot;,
                      trControl = XX,
                      preProcess = c(&quot;XX&quot;, &quot;XX&quot;),  # Standardise
                      tuneGrid = expand.grid(alpha = 0,  # Ridge penalty
                                             lambda = lambda_vec))</code></pre>
<pre class="r"><code># Ridge Regression --------------------------
salary_ridge &lt;- train(form = Salary ~ .,
                      data = hitters_train,
                      method = &quot;glmnet&quot;,
                      trControl = ctrl_cv,
                      preProcess = c(&quot;center&quot;, &quot;scale&quot;),  # Standardise
                      tuneGrid = expand.grid(alpha = 0,  # Ridge penalty
                                             lambda = lambda_vec))</code></pre>
<ol start="3" style="list-style-type: decimal">
<li>Print your <code>salary_ridge</code> object. What do you see?</li>
</ol>
<pre class="r"><code>salary_ridge</code></pre>
<pre><code>glmnet 

50 samples
19 predictors

Pre-processing: centered (19), scaled (19) 
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 46, 44, 44, 46, 45, 44, ... 
Resampling results across tuning parameters:

  lambda    RMSE  Rsquared  MAE
  1.00e-04  458   0.393     352
  1.20e-04  458   0.393     352
  1.45e-04  458   0.393     352
  1.75e-04  458   0.393     352
  2.10e-04  458   0.393     352
  2.54e-04  458   0.393     352
  3.05e-04  458   0.393     352
  3.68e-04  458   0.393     352
  4.43e-04  458   0.393     352
  5.34e-04  458   0.393     352
  6.43e-04  458   0.393     352
  7.74e-04  458   0.393     352
  9.33e-04  458   0.393     352
  1.12e-03  458   0.393     352
  1.35e-03  458   0.393     352
  1.63e-03  458   0.393     352
  1.96e-03  458   0.393     352
  2.36e-03  458   0.393     352
  2.85e-03  458   0.393     352
  3.43e-03  458   0.393     352
  4.13e-03  458   0.393     352
  4.98e-03  458   0.393     352
  5.99e-03  458   0.393     352
  7.22e-03  458   0.393     352
  8.70e-03  458   0.393     352
  1.05e-02  458   0.393     352
  1.26e-02  458   0.393     352
  1.52e-02  458   0.393     352
  1.83e-02  458   0.393     352
  2.21e-02  458   0.393     352
  2.66e-02  458   0.393     352
  3.20e-02  458   0.393     352
  3.85e-02  458   0.393     352
  4.64e-02  458   0.393     352
  5.59e-02  458   0.393     352
  6.73e-02  458   0.393     352
  8.11e-02  458   0.393     352
  9.77e-02  458   0.393     352
  1.18e-01  458   0.393     352
  1.42e-01  458   0.393     352
  1.71e-01  458   0.393     352
  2.06e-01  458   0.393     352
  2.48e-01  458   0.393     352
  2.98e-01  458   0.393     352
  3.59e-01  458   0.393     352
  4.33e-01  458   0.393     352
  5.21e-01  458   0.393     352
  6.28e-01  458   0.393     352
  7.56e-01  458   0.393     352
  9.11e-01  458   0.393     352
  1.10e+00  458   0.393     352
  1.32e+00  458   0.393     352
  1.59e+00  458   0.393     352
  1.92e+00  458   0.393     352
  2.31e+00  458   0.393     352
  2.78e+00  458   0.393     352
  3.35e+00  458   0.393     352
  4.04e+00  458   0.393     352
  4.86e+00  458   0.393     352
  5.86e+00  458   0.393     352
  7.05e+00  458   0.393     352
  8.50e+00  458   0.393     352
  1.02e+01  458   0.393     352
  1.23e+01  458   0.393     352
  1.48e+01  458   0.393     352
  1.79e+01  458   0.393     352
  2.15e+01  458   0.393     352
  2.60e+01  457   0.393     352
  3.13e+01  454   0.397     350
  3.76e+01  450   0.402     347
  4.53e+01  445   0.406     344
  5.46e+01  441   0.411     342
  6.58e+01  437   0.417     340
  7.92e+01  433   0.425     337
  9.55e+01  429   0.438     335
  1.15e+02  426   0.454     333
  1.38e+02  423   0.473     330
  1.67e+02  420   0.493     328
  2.01e+02  417   0.513     326
  2.42e+02  415   0.532     324
  2.92e+02  413   0.549     322
  3.51e+02  411   0.564     320
  4.23e+02  410   0.578     319
  5.09e+02  409   0.591     317
  6.14e+02  409   0.602     315
  7.39e+02  409   0.612     314
  8.90e+02  409   0.620     313
  1.07e+03  410   0.628     313
  1.29e+03  411   0.634     313
  1.56e+03  413   0.639     313
  1.87e+03  415   0.644     314
  2.26e+03  417   0.647     315
  2.72e+03  421   0.650     316
  3.27e+03  424   0.652     319
  3.94e+03  428   0.654     321
  4.75e+03  432   0.655     324
  5.72e+03  436   0.656     328
  6.89e+03  440   0.656     331
  8.30e+03  445   0.657     334
  1.00e+04  449   0.657     337

Tuning parameter &#39;alpha&#39; was held constant at a value of 0
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were alpha = 0 and lambda = 739.</code></pre>
<ol start="4" style="list-style-type: decimal">
<li>Plot your <code>salary_ridge</code> object. What do you see? Which value of the regularization parameter seems to be the best?</li>
</ol>
<pre class="r"><code># Plot salary_ridge object
plot(XX)</code></pre>
<pre class="r"><code>plot(salary_ridge)</code></pre>
<p><img src="Optimization_practical_files/figure-html/unnamed-chunk-25-1.png" width="576" style="display: block; margin: auto;" /></p>
<ol start="5" style="list-style-type: decimal">
<li>Print the best value of lambda by running the following code. Does this match what you saw in the plot above?</li>
</ol>
<pre class="r"><code># Print best regularisation parameter
salary_ridge$bestTune$lambda</code></pre>
<pre><code>[1] 739</code></pre>
<ol start="6" style="list-style-type: decimal">
<li>What were your final regression model coefficients for the best lambda value? Find them by running the following code.</li>
</ol>
<pre class="r"><code># Get coefficients from best lambda value
coef(salary_ridge$finalModel, 
     salary_ridge$bestTune$lambda)</code></pre>
<pre><code>20 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
                 1
(Intercept) 608.79
AtBat        -8.96
Hits          6.28
HmRun        -9.15
Runs          8.39
RBI           6.18
Walks        11.17
Years        12.10
CAtBat       30.42
CHits        32.50
CHmRun       26.36
CRuns        40.55
CRBI         35.48
CWalks       42.49
LeagueN      43.26
DivisionW   -42.79
PutOuts      15.77
Assists      41.40
Errors      -13.56
NewLeagueN   27.89</code></pre>
<ol start="7" style="list-style-type: decimal">
<li>How do these coefficients compare to what you found in regular regression? Are they similar? Different?</li>
</ol>
<pre class="r"><code># Actually the look quite different! The reason why is that we have changed the scale using preProcess

# If you want the coefficients on the original scale, you&#39;d need to convert them, or run your training again without any processing. However, this can lead to problems finding the optimal Lambda value...</code></pre>
</div>
<div id="e---lasso-regression" class="section level3">
<h3>E - Lasso Regression</h3>
<p>It’s time to fit an optimized regression model with a Lasso penalty!</p>
<ol style="list-style-type: decimal">
<li>Before we can fit a lasso regression model, we again first specify which values of the lambda penalty parameter we want to try. Using the code below, create a vector called <code>lambda_vec</code> which contains 100 values between 0 and 1,000.</li>
</ol>
<pre class="r"><code># Determine possible values of lambda
lambda_vec &lt;- 10 ^ seq(from = -4, to = 4, length = 100)</code></pre>
<ol start="2" style="list-style-type: decimal">
<li>Fit a lasso regression model predicting <code>Salary</code> as a function of all features. Specifically,…</li>
</ol>
<ul>
<li>set the formula to <code>Salary ~ .</code>.</li>
<li>set the data to <code>hitters_train</code>.</li>
<li>set the method to <code>&quot;glmnet&quot;</code> for regularized regression.</li>
<li>set the train control argument to <code>ctrl_cv</code>.</li>
<li>set the <code>preProcess</code> argument to <code>c(&quot;center&quot;, &quot;scale&quot;)</code> to make sure the variables are standarised when estimating the beta weights (this is also good practice for lasso regression).</li>
<li>set the tuneGrid argument such that alpha is 1 (for lasso regression), and with all lambda values you specified in <code>lambda_vec</code> (we’ve done this for you below).</li>
</ul>
<pre class="r"><code># Lasso Regression --------------------------
salary_lasso &lt;- train(form = XX ~ .,
                      data = XX,
                      method = &quot;XX&quot;,
                      trControl = XX,
                      preProcess = c(&quot;XX&quot;, &quot;XX&quot;),  # Standardise
                      tuneGrid = expand.grid(alpha = 1,  # Lasso penalty
                                            lambda = lambda_vec))</code></pre>
<pre class="r"><code># Fit a lasso regression
salary_lasso &lt;- train(form = Salary ~ .,
                   data = hitters_train,
                   method = &quot;glmnet&quot;,
                   trControl = ctrl_cv,
                   preProcess = c(&quot;center&quot;, &quot;scale&quot;),
                   tuneGrid = expand.grid(alpha = 1,  # Lasso penalty
                                          lambda = lambda_vec))</code></pre>
<ol start="3" style="list-style-type: decimal">
<li>Print your <code>salary_lasso</code> object. What do you see?</li>
</ol>
<pre class="r"><code>salary_lasso</code></pre>
<pre><code>glmnet 

50 samples
19 predictors

Pre-processing: centered (19), scaled (19) 
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 44, 46, 45, 45, 44, 45, ... 
Resampling results across tuning parameters:

  lambda    RMSE  Rsquared  MAE
  1.00e-04  546   0.222     434
  1.20e-04  546   0.222     434
  1.45e-04  546   0.222     434
  1.75e-04  546   0.222     434
  2.10e-04  546   0.222     434
  2.54e-04  546   0.222     434
  3.05e-04  546   0.222     434
  3.68e-04  546   0.222     434
  4.43e-04  546   0.222     434
  5.34e-04  546   0.222     434
  6.43e-04  546   0.222     434
  7.74e-04  546   0.222     434
  9.33e-04  546   0.222     434
  1.12e-03  546   0.222     434
  1.35e-03  546   0.222     434
  1.63e-03  546   0.222     434
  1.96e-03  546   0.222     434
  2.36e-03  546   0.222     434
  2.85e-03  546   0.222     434
  3.43e-03  546   0.222     434
  4.13e-03  546   0.222     434
  4.98e-03  546   0.222     434
  5.99e-03  546   0.222     434
  7.22e-03  546   0.222     434
  8.70e-03  546   0.222     434
  1.05e-02  546   0.222     434
  1.26e-02  546   0.222     434
  1.52e-02  546   0.222     434
  1.83e-02  546   0.222     434
  2.21e-02  546   0.222     434
  2.66e-02  546   0.222     434
  3.20e-02  546   0.222     434
  3.85e-02  546   0.222     434
  4.64e-02  546   0.223     434
  5.59e-02  546   0.223     433
  6.73e-02  546   0.224     433
  8.11e-02  545   0.224     432
  9.77e-02  545   0.225     432
  1.18e-01  545   0.226     431
  1.42e-01  544   0.227     430
  1.71e-01  544   0.229     429
  2.06e-01  543   0.231     428
  2.48e-01  543   0.234     426
  2.98e-01  542   0.237     424
  3.59e-01  542   0.241     422
  4.33e-01  541   0.245     421
  5.21e-01  540   0.251     420
  6.28e-01  537   0.258     418
  7.56e-01  535   0.264     416
  9.11e-01  531   0.262     412
  1.10e+00  524   0.257     408
  1.32e+00  515   0.258     400
  1.59e+00  506   0.258     393
  1.92e+00  500   0.264     389
  2.31e+00  493   0.284     384
  2.78e+00  484   0.318     376
  3.35e+00  475   0.355     368
  4.04e+00  466   0.374     361
  4.86e+00  460   0.378     358
  5.86e+00  455   0.373     354
  7.05e+00  449   0.361     350
  8.50e+00  444   0.369     348
  1.02e+01  440   0.375     346
  1.23e+01  436   0.388     344
  1.48e+01  434   0.402     342
  1.79e+01  431   0.418     338
  2.15e+01  426   0.434     335
  2.60e+01  422   0.451     331
  3.13e+01  417   0.468     328
  3.76e+01  411   0.488     324
  4.53e+01  407   0.501     320
  5.46e+01  409   0.516     321
  6.58e+01  412   0.526     323
  7.92e+01  418   0.529     327
  9.55e+01  422   0.526     328
  1.15e+02  423   0.516     328
  1.38e+02  426   0.491     330
  1.67e+02  428   0.483     332
  2.01e+02  438   0.483     339
  2.42e+02  454   0.450     349
  2.92e+02  468   0.152     365
  3.51e+02  468     NaN     368
  4.23e+02  468     NaN     368
  5.09e+02  468     NaN     368
  6.14e+02  468     NaN     368
  7.39e+02  468     NaN     368
  8.90e+02  468     NaN     368
  1.07e+03  468     NaN     368
  1.29e+03  468     NaN     368
  1.56e+03  468     NaN     368
  1.87e+03  468     NaN     368
  2.26e+03  468     NaN     368
  2.72e+03  468     NaN     368
  3.27e+03  468     NaN     368
  3.94e+03  468     NaN     368
  4.75e+03  468     NaN     368
  5.72e+03  468     NaN     368
  6.89e+03  468     NaN     368
  8.30e+03  468     NaN     368
  1.00e+04  468     NaN     368

Tuning parameter &#39;alpha&#39; was held constant at a value of 1
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were alpha = 1 and lambda = 45.3.</code></pre>
<ol start="4" style="list-style-type: decimal">
<li>Plot your <code>salary_lasso</code> object. What do you see? Which value of the regularization parameter seems to be the best?</li>
</ol>
<pre class="r"><code># Plot salary_lasso object
plot(XX)</code></pre>
<pre class="r"><code>plot(salary_lasso)</code></pre>
<p><img src="Optimization_practical_files/figure-html/unnamed-chunk-34-1.png" width="576" style="display: block; margin: auto;" /></p>
<ol start="5" style="list-style-type: decimal">
<li>Print the best value of lambda by running the following code. Does this match what you saw in the plot above?</li>
</ol>
<pre class="r"><code># Print best regularisation parameter
salary_lasso$bestTune$lambda</code></pre>
<pre><code>[1] 45.3</code></pre>
<ol start="6" style="list-style-type: decimal">
<li>What were your final regression model coefficients for the best lambda value? Find them by running the following code.</li>
</ol>
<pre class="r"><code># Get coefficients from best lambda value
coef(salary_lasso$finalModel, 
     salary_lasso$bestTune$lambda)</code></pre>
<pre><code>20 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
                1
(Intercept) 608.8
AtBat         .  
Hits          .  
HmRun         .  
Runs          .  
RBI           .  
Walks         .  
Years         .  
CAtBat        .  
CHits         .  
CHmRun        .  
CRuns       134.2
CRBI          .  
CWalks      108.9
LeagueN      88.9
DivisionW   -58.1
PutOuts       .  
Assists      34.1
Errors        .  
NewLeagueN    .  </code></pre>
<ol start="7" style="list-style-type: decimal">
<li>How do these coefficients compare to what you found in regular regression? Are they similar? Different? Do you notice that any coefficients are now set to exactly 0?</li>
</ol>
<pre class="r"><code># Yep!

# I see that many features are now removed!</code></pre>
</div>
<div id="f---decision-tree" class="section level3">
<h3>F - Decision Tree</h3>
<p>It’s time to fit an optimized decision tree model!</p>
<ol style="list-style-type: decimal">
<li>Before we can fit a decision tree, we need to specify which values of the complexity parameter <code>cp</code> we want to try. Using the code below, create a vector called <code>cp_vec</code> which contains 100 values between 0 and 1.</li>
</ol>
<pre class="r"><code># Determine possible values of the complexity parameter cp
cp_vec &lt;- seq(from = 0, to = 1, length = 100)</code></pre>
<ol start="2" style="list-style-type: decimal">
<li>Fit a decision tree model predicting <code>Salary</code> as a function of all features. Specifically,…</li>
</ol>
<ul>
<li>set the formula to <code>Salary ~ .</code>.</li>
<li>set the data to <code>hitters_train</code>.</li>
<li>set the method to <code>&quot;rpart&quot;</code> for decision trees.</li>
<li>set the train control argument to <code>ctrl_cv</code>,</li>
<li>set the tuneGrid argument with all <code>cp</code> values you specified in <code>cp_vec</code>.</li>
</ul>
<pre class="r"><code># Decision Tree --------------------------
salary_rpart &lt;- train(form = XX ~ .,
                  data = XX,
                  method = &quot;XX&quot;,
                  trControl = XX,
                  tuneGrid = expand.grid(cp = cp_vec))</code></pre>
<pre class="r"><code># Decision Tree --------------------------
salary_rpart &lt;- train(form = Salary ~ .,
                  data = hitters_train,
                  method = &quot;rpart&quot;,
                  trControl = ctrl_cv,
                  tuneGrid = expand.grid(cp = cp_vec))</code></pre>
<ol start="3" style="list-style-type: decimal">
<li>Print your <code>salary_rpart</code> object. What do you see?</li>
</ol>
<pre class="r"><code>salary_rpart</code></pre>
<pre><code>CART 

50 samples
19 predictors

No pre-processing
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 46, 44, 44, 44, 44, 46, ... 
Resampling results across tuning parameters:

  cp      RMSE  Rsquared  MAE
  0.0000  464   0.3054    354
  0.0101  464   0.3054    354
  0.0202  464   0.3054    354
  0.0303  464   0.3054    354
  0.0404  470   0.2618    363
  0.0505  470   0.2618    363
  0.0606  488   0.2151    384
  0.0707  487   0.2010    386
  0.0808  487   0.2170    382
  0.0909  498   0.2223    395
  0.1010  507   0.2092    408
  0.1111  508   0.1906    409
  0.1212  508   0.1906    409
  0.1313  508   0.1906    409
  0.1414  503   0.1906    404
  0.1515  505   0.1906    411
  0.1616  505   0.1906    411
  0.1717  505   0.1906    411
  0.1818  505   0.1906    411
  0.1919  505   0.1906    411
  0.2020  505   0.1906    411
  0.2121  505   0.1906    411
  0.2222  505   0.1906    411
  0.2323  505   0.1906    411
  0.2424  505   0.1906    411
  0.2525  505   0.1906    411
  0.2626  505   0.1906    411
  0.2727  525   0.1226    423
  0.2828  535   0.1255    417
  0.2929  535   0.1255    417
  0.3030  522   0.0618    409
  0.3131  522   0.0618    409
  0.3232  522   0.0618    409
  0.3333  522   0.0618    409
  0.3434  506   0.0320    393
  0.3535  506   0.0320    393
  0.3636  506   0.0320    393
  0.3737  506   0.0320    393
  0.3838  506   0.0320    393
  0.3939  506   0.0320    393
  0.4040  506   0.0320    393
  0.4141  506   0.0320    393
  0.4242  506   0.0320    393
  0.4343  506   0.0320    393
  0.4444  506   0.0320    393
  0.4545  506   0.0320    393
  0.4646  494      NaN    387
  0.4747  494      NaN    387
  0.4848  494      NaN    387
  0.4949  494      NaN    387
  0.5051  494      NaN    387
  0.5152  494      NaN    387
  0.5253  494      NaN    387
  0.5354  494      NaN    387
  0.5455  494      NaN    387
  0.5556  494      NaN    387
  0.5657  494      NaN    387
  0.5758  494      NaN    387
  0.5859  494      NaN    387
  0.5960  494      NaN    387
  0.6061  494      NaN    387
  0.6162  494      NaN    387
  0.6263  494      NaN    387
  0.6364  494      NaN    387
  0.6465  494      NaN    387
  0.6566  494      NaN    387
  0.6667  494      NaN    387
  0.6768  494      NaN    387
  0.6869  494      NaN    387
  0.6970  494      NaN    387
  0.7071  494      NaN    387
  0.7172  494      NaN    387
  0.7273  494      NaN    387
  0.7374  494      NaN    387
  0.7475  494      NaN    387
  0.7576  494      NaN    387
  0.7677  494      NaN    387
  0.7778  494      NaN    387
  0.7879  494      NaN    387
  0.7980  494      NaN    387
  0.8081  494      NaN    387
  0.8182  494      NaN    387
  0.8283  494      NaN    387
  0.8384  494      NaN    387
  0.8485  494      NaN    387
  0.8586  494      NaN    387
  0.8687  494      NaN    387
  0.8788  494      NaN    387
  0.8889  494      NaN    387
  0.8990  494      NaN    387
  0.9091  494      NaN    387
  0.9192  494      NaN    387
  0.9293  494      NaN    387
  0.9394  494      NaN    387
  0.9495  494      NaN    387
  0.9596  494      NaN    387
  0.9697  494      NaN    387
  0.9798  494      NaN    387
  0.9899  494      NaN    387
  1.0000  494      NaN    387

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was cp = 0.0303.</code></pre>
<ol start="4" style="list-style-type: decimal">
<li>Plot your <code>salary_rpart</code> object. What do you see? Which value of the complexity parameter seems to be the best?</li>
</ol>
<pre class="r"><code># Plot salary_rpart object
plot(XX)</code></pre>
<pre class="r"><code>plot(salary_rpart)</code></pre>
<p><img src="Optimization_practical_files/figure-html/unnamed-chunk-43-1.png" width="576" style="display: block; margin: auto;" /></p>
<ol start="5" style="list-style-type: decimal">
<li>Print the best value of <code>cp</code> by running the following code. Does this match what you saw in the plot above?</li>
</ol>
<pre class="r"><code># Print best regularisation parameter
salary_rpart$bestTune$cp</code></pre>
<pre><code>[1] 0.0303</code></pre>
<ol start="6" style="list-style-type: decimal">
<li>Plot your final decision tree using the following code:</li>
</ol>
<pre class="r"><code># Visualise your trees
plot(as.party(salary_rpart$finalModel)) </code></pre>
<p><img src="Optimization_practical_files/figure-html/unnamed-chunk-45-1.png" width="576" style="display: block; margin: auto;" /></p>
<ol start="7" style="list-style-type: decimal">
<li>How do the nodes in the tree compare to the coefficients you found in your regression analyses? Do you see similarities or differences?</li>
</ol>
<pre class="r"><code># Actually, the tree just has one root and no nodes! This is due to the optimal complexity parameter being so high.</code></pre>
</div>
<div id="g---random-forests" class="section level3">
<h3>G - Random Forests</h3>
<p>It’s time to fit an optimized random forest model!</p>
<ol style="list-style-type: decimal">
<li>Before we can fit a random forest model, we need to specify which values of the diversity parameter <code>mtry</code> we want to try. Using the code below, create a vector called <code>mtry_vec</code> which is a sequence of numbers from 1 to 10.</li>
</ol>
<pre class="r"><code># Determine possible values of the random forest diversity parameter mtry
mtry_vec &lt;- 1:10</code></pre>
<ol start="2" style="list-style-type: decimal">
<li>Fit a random forest model predicting <code>Salary</code> as a function of all features. Specifically,…</li>
</ol>
<ul>
<li>set the formula to <code>Salary ~ .</code>.</li>
<li>set the data to <code>hitters_train</code>.</li>
<li>set the method to <code>&quot;rf&quot;</code> for random forests.</li>
<li>set the train control argument to <code>ctrl_cv</code>.</li>
<li>set the tuneGrid argument such that mtry can take on the values you defined in <code>mtry_vec</code>.</li>
</ul>
<pre class="r"><code># Random Forest --------------------------
salary_rf &lt;- train(form = XX ~ .,
                   data = XX,
                   method = &quot;XX&quot;,
                   trControl = XX,
                   tuneGrid = expand.grid(mtry = mtry_vec))</code></pre>
<pre class="r"><code># Random Forest --------------------------
salary_rf &lt;- train(form = Salary ~ .,
                   data = hitters_train,
                   method = &quot;rf&quot;,
                   trControl = ctrl_cv,
                   tuneGrid = expand.grid(mtry = mtry_vec))</code></pre>
<ol start="3" style="list-style-type: decimal">
<li>Print your <code>salary_rf</code> object. What do you see?</li>
</ol>
<pre class="r"><code>salary_rf</code></pre>
<pre><code>Random Forest 

50 samples
19 predictors

No pre-processing
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 46, 45, 46, 45, 45, 45, ... 
Resampling results across tuning parameters:

  mtry  RMSE  Rsquared  MAE
   1    353   0.608     259
   2    339   0.628     246
   3    333   0.634     240
   4    331   0.634     240
   5    331   0.628     241
   6    332   0.630     240
   7    331   0.627     239
   8    329   0.623     238
   9    329   0.623     238
  10    328   0.614     238

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was mtry = 10.</code></pre>
<ol start="4" style="list-style-type: decimal">
<li>Plot your <code>salary_rf</code> object. What do you see? Which value of the regularization parameter seems to be the best?</li>
</ol>
<pre class="r"><code># Plot salary_rf object
plot(XX)</code></pre>
<pre class="r"><code>plot(salary_rf)</code></pre>
<p><img src="Optimization_practical_files/figure-html/unnamed-chunk-52-1.png" width="576" style="display: block; margin: auto;" /></p>
<ol start="5" style="list-style-type: decimal">
<li>Print the best value of <code>mtry</code> by running the following code. Does this match what you saw in the plot above?</li>
</ol>
<pre class="r"><code># Print best mtry parameter
salary_rf$bestTune$mtry</code></pre>
<pre><code>[1] 10</code></pre>
</div>
<div id="h---estimate-prediction-accuracy-from-training-folds" class="section level3">
<h3>H - Estimate prediction accuracy from training folds</h3>
<ol style="list-style-type: decimal">
<li>Using <code>resamples()</code>, calculate the estimated prediction accuracy for each of your models. To do this, put your model objects in the named list (e.g.; <code>glm = salary_glm</code>, …). See below.</li>
</ol>
<pre class="r"><code># Summarise accuracy statistics across folds
salary_resamples &lt;- resamples(list(glm = XXX,
                                   ridge = XXX, 
                                   lasso = XXX, 
                                   rpart = XXX, 
                                   rf = XXX))</code></pre>
<pre class="r"><code># Summarise accuracy statistics across folds
salary_resamples &lt;- resamples(list(glm = salary_glm,
                                   ridge = salary_ridge, 
                                   lasso = salary_lasso, 
                                   dt = salary_rpart, 
                                   rf = salary_rf))</code></pre>
<ol start="2" style="list-style-type: decimal">
<li>Look at the summary of your <code>salary_resamples</code> object with <code>summary(salary_resamples)</code>. What does this tell you? Which model do you expect to have the best prediction accuracy for the test data?</li>
</ol>
<pre class="r"><code># Print summaries of cross-validation accuracy 

# I see below that the random forest model has the lowest mean MAE, so I
#  would expect it to be the best model in the true test data
summary(salary_resamples)</code></pre>
<pre><code>
Call:
summary.resamples(object = salary_resamples)

Models: glm, ridge, lasso, dt, rf 
Number of resamples: 10 

MAE 
      Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s
glm    253     377    497  490     559  743    0
ridge  147     211    319  314     354  596    0
lasso  187     216    297  320     386  514    0
dt     170     262    396  354     423  500    0
rf      45     175    273  238     318  336    0

RMSE 
       Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s
glm   274.8     490    533  575     735  901    0
ridge 176.5     257    387  409     477  916    0
lasso 211.2     233    379  407     458  893    0
dt    267.0     381    429  464     491  812    0
rf     47.2     228    330  328     382  625    0

Rsquared 
          Min. 1st Qu. Median  Mean 3rd Qu.  Max. NA&#39;s
glm   0.013772  0.1228  0.203 0.377   0.701 0.893    0
ridge 0.131450  0.4330  0.671 0.612   0.808 0.973    0
lasso 0.017893  0.1669  0.581 0.501   0.784 0.965    0
dt    0.000986  0.0235  0.156 0.305   0.463 0.955    0
rf    0.122475  0.3378  0.740 0.614   0.783 0.981    0</code></pre>
</div>
<div id="i---calculate-prediction-accuracy" class="section level3">
<h3>I - Calculate prediction accuracy</h3>
<ol style="list-style-type: decimal">
<li>Save the criterion value for the test data as a new vector called <code>criterion_test</code>.</li>
</ol>
<pre class="r"><code># Save salaries of players in test dataset as criterion_test
criterion_test &lt;- XXX$XXX</code></pre>
<pre class="r"><code>criterion_test &lt;- hitters_test$Salary</code></pre>
<ol start="2" style="list-style-type: decimal">
<li>Using <code>predict()</code>, save the prediction of your regular regression model <code>salary_glm</code> for the <code>hitters_test</code> data as a new object called <code>glm_pred</code>. Specifically,…</li>
</ol>
<ul>
<li>set the first argument to <code>salary_glm</code>.</li>
<li>set the newdata argument to <code>hitters_test</code>.</li>
</ul>
<pre class="r"><code># Save the glm predicted salaries of hitters_test
glm_pred &lt;- predict(XXX, newdata = XXX)</code></pre>
<pre class="r"><code># Save predictions for glm
glm_pred &lt;- predict(salary_glm, newdata = hitters_test)</code></pre>
<ol start="3" style="list-style-type: decimal">
<li>Now do the same with your ridge, lasso, decision tree, and random forest models to get the objects <code>ridge_pred</code>, <code>lasso_pred</code>, <code>rpart_pred</code> and <code>rf_pred</code>.</li>
</ol>
<pre class="r"><code>ridge_pred &lt;- predict(XXX, newdata = XXX)
lasso_pred &lt;- predict(XXX, newdata = XXX)
rpart_pred &lt;- predict(XXX, newdata = XXX)
rf_pred &lt;- predict(XXX, newdata = XXX)</code></pre>
<pre class="r"><code># Save predictions from other models
ridge_pred &lt;- predict(salary_ridge, newdata = hitters_test)
lasso_pred &lt;- predict(salary_lasso, newdata = hitters_test)
rpart_pred &lt;- predict(salary_rpart, newdata = hitters_test)
rf_pred &lt;- predict(salary_rf, newdata = hitters_test)</code></pre>
<ol start="4" style="list-style-type: decimal">
<li>Using <code>postResample()</code>, calculate the aggregate prediction accuracy for each model using the template below. Specifically,…</li>
</ol>
<ul>
<li>set the pred argument to your model predictions (e.g.; <code>ridge_pred</code>).</li>
<li>set the <code>obs</code> argument to the true criterion values <code>criterion_test</code>).</li>
</ul>
<pre class="r"><code># Calculate aggregate accuracy for a model
postResample(pred = XXX, 
             obs = XXX)</code></pre>
<pre class="r"><code># Prediction error for normal regression
postResample(pred = glm_pred, 
             obs = hitters_test$Salary)</code></pre>
<pre><code>    RMSE Rsquared      MAE 
552.1782   0.0907 421.8898 </code></pre>
<pre class="r"><code># Prediction error for ridge regression
postResample(pred = ridge_pred, 
             obs = hitters_test$Salary)</code></pre>
<pre><code>    RMSE Rsquared      MAE 
 366.712    0.329  274.623 </code></pre>
<pre class="r"><code># Prediction error for lasso regression
postResample(pred = lasso_pred, 
             obs = hitters_test$Salary)</code></pre>
<pre><code>    RMSE Rsquared      MAE 
 389.565    0.254  286.021 </code></pre>
<pre class="r"><code># Prediction error for decision trees
postResample(pred = rpart_pred, 
             obs = hitters_test$Salary)</code></pre>
<pre><code>    RMSE Rsquared      MAE 
 359.273    0.358  269.471 </code></pre>
<pre class="r"><code># Prediction error for random forests
postResample(pred = rf_pred, 
             obs = hitters_test$Salary)</code></pre>
<pre><code>    RMSE Rsquared      MAE 
 301.353    0.531  199.995 </code></pre>
<ol start="5" style="list-style-type: decimal">
<li>Which of your models had the best performance in the true test data?</li>
</ol>
<pre class="r"><code># Random forests had the lowest test MAE of
postResample(pred = rf_pred, 
             obs = hitters_test$Salary)[3]</code></pre>
<pre><code>MAE 
200 </code></pre>
<ol start="6" style="list-style-type: decimal">
<li>How close were your models’ true prediction error to the values you estimated in the previous section when you ran <code>resamples()</code>?</li>
</ol>
<pre class="r"><code># Depends on what you mean by &#39;close&#39;, but they are definitely higher (worse) in the test data</code></pre>
</div>
<div id="z---challenges" class="section level3">
<h3>Z - Challenges</h3>
<ol style="list-style-type: decimal">
<li>In addition to ‘regular’ 10 fold cross-validation, you can also do repeated 10-fold cross-validation, where the cross validation procedure is repeated many times. Do you think this will improve your models’ performance? To test this, create a new training control object called <code>ctrl_cv_rep</code> as below. Then, train your models again using <code>ctrl_cv_rep</code> (instead of <code>ctrl_cv</code>), and evaluate their prediction performance. Do they improve? Do you get different optimal tuning values compared to your previous models?</li>
</ol>
<pre class="r"><code># Repeated cross validation.
#  Folds = 10
#  Repeats = 5
ctrl_cv_rep &lt;- trainControl(method = &quot;repeatedcv&quot;,
                            number = 10,
                            repeats = 5)</code></pre>
<ol start="2" style="list-style-type: decimal">
<li><p>Using the same procedure as above, compare models predicting the prices of houses in King County Washington using the <code>house_train</code> and <code>house_test</code> datasets.</p></li>
<li><p>When using lasso regression, do you find that the lasso sets any beta weights to exactly 0? If so, which ones?</p></li>
<li><p>Which model does the best and how accurate was it? Was it the same model that performed the best predicting baseball player salaries?</p></li>
<li><p>Using the same procedure as above, compare models predicting the graduate rate of students from different colleges using the <code>college_train</code> and <code>college_test</code> datasets.</p></li>
<li><p>When using lasso regression, do you find that the lasso sets any beta weights to exactly 0? If so, which ones?</p></li>
<li><p>Which model does the best and how accurate was it? Was it the same model that performed the best predicting baseball player salaries?</p></li>
</ol>
</div>
</div>
<div id="examples" class="section level2">
<h2>Examples</h2>
<pre class="r"><code># Model optimization with Regression

# Step 0: Load packages-----------
library(tidyverse)    # Load tidyverse for dplyr and tidyr
library(caret)        # For ML mastery 
library(partykit)     # For decision trees
library(party)        # For decision trees

# Step 1: Load, clean, and explore data ----------------------

# training data
data_train &lt;- read_csv(&quot;1_Data/diamonds_train.csv&quot;)

# test data
data_test &lt;- read_csv(&quot;1_Data/diamonds_test.csv&quot;)

# Convert all characters to factor
#  Some ML models require factors
data_train &lt;- data_train %&gt;%
  mutate_if(is.character, factor)

data_test &lt;- data_test %&gt;%
  mutate_if(is.character, factor)

# Explore training data
data_train        # Print the dataset
View(data_train)  # Open in a new spreadsheet-like window 
dim(data_train)   # Print dimensions
names(data_train) # Print the names

# Define criterion_train
#   We&#39;ll use this later to evaluate model accuracy
criterion_train &lt;- data_train$price
criterion_test &lt;- data_test$price

# Step 2: Define training control parameters -------------

# Use 10-fold cross validation
ctrl_cv &lt;- trainControl(method = &quot;cv&quot;, 
                        number = 10) 

# Step 3: Train models: -----------------------------
#   Criterion: hwy
#   Features: year, cyl, displ

# Normal Regression --------------------------
price_glm &lt;- train(form = price ~ carat + depth + table + x + y,
                   data = data_train,
                   method = &quot;glm&quot;,
                   trControl = ctrl_cv)


# Print key results
price_glm

# RMSE  Rsquared  MAE
# 1506  0.86      921

# Coefficients
coef(price_glm$finalModel)

# (Intercept)       carat       depth       table           x           y 
#     21464.9     11040.4      -215.6       -94.2     -3681.6      2358.9 

# Lasso --------------------------

# Vector of lambda values to try
lambda_vec &lt;- 10 ^ seq(-3, 3, length = 100)

price_lasso &lt;- train(form = price ~ carat + depth + table + x + y,
                   data = data_train,
                   method = &quot;glmnet&quot;,
                   trControl = ctrl_cv,
                   preProcess = c(&quot;center&quot;, &quot;scale&quot;),  # Standardise
                   tuneGrid = expand.grid(alpha = 1,  # Lasso
                                          lambda = lambda_vec))


# Print key results
price_lasso

# glmnet 
# 
# 5000 samples
#    5 predictor
# 
# Pre-processing: centered (5), scaled (5) 
# Resampling: Cross-Validated (10 fold) 
# Summary of sample sizes: 4500, 4500, 4500, 4500, 4500, 4500, ... 
# Resampling results across tuning parameters:
# 
#   lambda    RMSE  Rsquared  MAE 
#   1.00e-03  1509  0.858      918
#   1.15e-03  1509  0.858      918
#   1.32e-03  1509  0.858      918
#   1.52e-03  1509  0.858      918

# Plot regularisation parameter versus error
plot(price_lasso)

# Print best regularisation parameter
price_lasso$bestTune$lambda

# Get coefficients from best lambda value
coef(price_lasso$finalModel, 
     price_lasso$bestTune$lambda)

# 6 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
#                 1
# (Intercept)  4001
# carat        5179
# depth        -300
# table        -213
# x           -3222
# y            1755


# Ridge --------------------------

# Vector of lambda values to try
lambda_vec &lt;- 10 ^ seq(-3, 3, length = 100)

price_ridge &lt;- train(form = price ~ carat + depth + table + x + y,
                     data = data_train,
                     method = &quot;glmnet&quot;,
                     trControl = ctrl_cv,
                     preProcess = c(&quot;center&quot;, &quot;scale&quot;),  # Standardise
                     tuneGrid = expand.grid(alpha = 0,  # Ridge penalty
                                            lambda = lambda_vec))

# Print key results
price_ridge

# glmnet 
# 
# 5000 samples
#    5 predictor
# 
# Pre-processing: centered (5), scaled (5) 
# Resampling: Cross-Validated (10 fold) 
# Summary of sample sizes: 4500, 4500, 4500, 4500, 4500, 4500, ... 
# Resampling results across tuning parameters:
# 
#   lambda    RMSE  Rsquared  MAE 
#   1.00e-03  1638  0.835     1137
#   1.15e-03  1638  0.835     1137
#   1.32e-03  1638  0.835     1137
#   1.52e-03  1638  0.835     1137
#   1.75e-03  1638  0.835     1137

# Plot regularisation parameter versus error
plot(price_ridge)

# Print best regularisation parameter
price_ridge$bestTune$lambda

# Get coefficients from best lambda value
coef(price_ridge$finalModel, 
     price_ridge$bestTune$lambda)

# 6 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
#                1
# (Intercept) 4001
# carat       2059
# depth       -131
# table       -168
# x            716
# y            797

# Decision Trees --------------------------

# Vector of cp values to try
cp_vec &lt;- seq(0, .1, length = 100)

price_rpart &lt;- train(form = price ~ carat + depth + table + x + y,
                  data = data_train,
                  method = &quot;rpart&quot;,
                  trControl = ctrl_cv,
                  tuneGrid = expand.grid(cp = cp_vec))

# Print key results
price_rpart

# Plot complexity parameter vs. error
plot(price_rpart)

# Print best complexity parameter
price_rpart$bestTune$cp

# [1] 0.00202

# Step 3: Estimate prediction accuracy from folds ----

# Get accuracy statistics across folds
resamples_price &lt;- resamples(list(ridge = price_ridge, 
                                  lasso = price_lasso, 
                                  glm = price_glm))

# Print summary of accuracies
summary(resamples_price)

# MAE 
#       Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s
# ridge 1094    1100   1117 1137    1170 1217    0
# lasso  869     887    929  918     944  960    0
# glm    856     882    921  920     949  986    0
# 
# RMSE 
#       Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s
# ridge 1545    1580   1609 1638    1703 1772    0
# lasso 1323    1479   1518 1509    1583 1593    0
# glm   1350    1429   1526 1509    1582 1702    0
# 
# Rsquared 
#        Min. 1st Qu. Median  Mean 3rd Qu.  Max. NA&#39;s
# ridge 0.798   0.828  0.836 0.835   0.848 0.854    0
# lasso 0.827   0.846  0.858 0.858   0.868 0.902    0
# glm   0.819   0.849  0.863 0.860   0.870 0.888    0

# Step 4: Measure prediction Accuracy -------------------

# GLM ================================

# Predictions
glm_pred &lt;- predict(price_glm, 
                    newdata = data_test)

# Calculate aggregate accuracy
postResample(pred = glm_pred, 
             obs = criterion_test)

#     RMSE Rsquared      MAE 
# 1654.017    0.832  944.854 

# Ridge ================================

# Predictions
ridge_pred &lt;- predict(price_ridge, 
                      newdata = data_test)

# Calculate aggregate accuracy
postResample(pred = ridge_pred, 
             obs = criterion_test)

#     RMSE Rsquared      MAE 
# 1650.541    0.832 1133.063 


# Lasso ================================

# Predictions
lasso_pred &lt;- predict(price_lasso, 
                      newdata = data_test)

# Calculate aggregate accuracy
postResample(pred = lasso_pred, 
             obs = criterion_test)

#     RMSE Rsquared      MAE 
# 1653.675    0.832  942.870 


# Visualise Accuracy -------------------------

# Tidy competition results
accuracy &lt;- tibble(criterion_test = criterion_test,
                   Regression = glm_pred,
                   Ridge = ridge_pred,
                   Lasso = lasso_pred) %&gt;%
               gather(model, prediction, -criterion_test) %&gt;%
               # Add error measures
               mutate(se = prediction - criterion_test,
                      ae = abs(prediction - criterion_test))

# Calculate summaries
accuracy_agg &lt;- accuracy %&gt;%
                  group_by(model) %&gt;%
                  summarise(mae = mean(ae))   # Calculate MAE (mean absolute error)

# Plot A) Scatterplot of truth versus predictions
ggplot(data = accuracy,
       aes(x = criterion_test, y = prediction, col = model)) +
  geom_point(alpha = .5) +
  geom_abline(slope = 1, intercept = 0) +
  labs(x = &quot;True Prices&quot;,
       y = &quot;Predicted Prices&quot;,
       title = &quot;Predicting Diamond Prices&quot;,
       subtitle = &quot;Black line indicates perfect performance&quot;)

# Plot B) Violin plot of absolute errors
ggplot(data = accuracy, 
       aes(x = model, y = ae, fill = model)) + 
  geom_violin() + 
  geom_jitter(width = .05, alpha = .2) +
  labs(title = &quot;Fitting Absolute Errors&quot;,
       subtitle = &quot;Numbers indicate means&quot;,
       x = &quot;Model&quot;,
       y = &quot;Absolute Error (Log Transformed)&quot;) +
  guides(fill = FALSE) +
  annotate(geom = &quot;label&quot;, 
           x = accuracy_agg$model, 
           y = accuracy_agg$mae, 
           label = round(accuracy_agg$mae, 2)) +
  scale_y_continuous(trans=&#39;log&#39;)</code></pre>
</div>
<div id="datasets" class="section level2">
<h2>Datasets</h2>
<table>
<thead>
<tr class="header">
<th align="left">File</th>
<th align="left">Rows</th>
<th align="left">Columns</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><a href="https://raw.githubusercontent.com/therbootcamp/ML_2019Oct/master/1_Data/hitters_train.csv">hitters_train.csv</a></td>
<td align="left">50</td>
<td align="left">20</td>
</tr>
<tr class="even">
<td align="left"><a href="https://raw.githubusercontent.com/therbootcamp/ML_2019Oct/master/1_Data/hitters_test.csv">hitters_test.csv</a></td>
<td align="left">213</td>
<td align="left">20</td>
</tr>
<tr class="odd">
<td align="left"><a href="https://raw.githubusercontent.com/therbootcamp/ML_2019Oct/master/1_Data/college_train.csv">college_train.csv</a></td>
<td align="left">500</td>
<td align="left">18</td>
</tr>
<tr class="even">
<td align="left"><a href="https://raw.githubusercontent.com/therbootcamp/ML_2019Oct/master/1_Data/college_test.csv">college_test.csv</a></td>
<td align="left">277</td>
<td align="left">18</td>
</tr>
<tr class="odd">
<td align="left"><a href="https://raw.githubusercontent.com/therbootcamp/ML_2019Oct/master/1_Data/house_train.csv">house_train.csv</a></td>
<td align="left">5000</td>
<td align="left">21</td>
</tr>
<tr class="even">
<td align="left"><a href="https://raw.githubusercontent.com/therbootcamp/ML_2019Oct/master/1_Data/hitters_test.csv">house_test.csv</a></td>
<td align="left">1000</td>
<td align="left">21</td>
</tr>
</tbody>
</table>
<ul>
<li><p>The <code>hitters_train</code> and <code>hitters_test</code> data are taken from the <code>Hitters</code> dataset in the <code>ISLR</code> package. They are data frames with observations of major league baseball players from the 1986 and 1987 seasons.</p></li>
<li><p>The <code>college_train</code> and <code>college_test</code> data are taken from the <code>College</code> dataset in the <code>ISLR</code> package. They contain statistics for a large number of US Colleges from the 1995 issue of US News and World Report.</p></li>
<li><p>The <code>house_train</code> and <code>house_test</code> data come from <a href="https://www.kaggle.com/harlfoxem/housesalesprediction">https://www.kaggle.com/harlfoxem/housesalesprediction</a></p></li>
</ul>
<div id="variable-description-of-hitters_train-and-hitters_test" class="section level4">
<h4>Variable description of <code>hitters_train</code> and <code>hitters_test</code></h4>
<table>
<colgroup>
<col width="26%" />
<col width="73%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Name</th>
<th align="left">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><code>Salary</code></td>
<td align="left">1987 annual salary on opening day in thousands of dollars.</td>
</tr>
<tr class="even">
<td align="left"><code>AtBat</code></td>
<td align="left">Number of times at bat in 1986.</td>
</tr>
<tr class="odd">
<td align="left"><code>Hits</code></td>
<td align="left">Number of hits in 1986.</td>
</tr>
<tr class="even">
<td align="left"><code>HmRun</code></td>
<td align="left">Number of home runs in 1986.</td>
</tr>
<tr class="odd">
<td align="left"><code>Runs</code></td>
<td align="left">Number of runs in 1986.</td>
</tr>
<tr class="even">
<td align="left"><code>RBI</code></td>
<td align="left">Number of runs batted in in 1986.</td>
</tr>
<tr class="odd">
<td align="left"><code>Walks</code></td>
<td align="left">Number of walks in 1986.</td>
</tr>
<tr class="even">
<td align="left"><code>Years</code></td>
<td align="left">Number of years in the major leagues.</td>
</tr>
<tr class="odd">
<td align="left"><code>CAtBat</code></td>
<td align="left">Number of times at bat during his career.</td>
</tr>
<tr class="even">
<td align="left"><code>CHits</code></td>
<td align="left">Number of hits during his career.</td>
</tr>
<tr class="odd">
<td align="left"><code>CHmRun</code></td>
<td align="left">Number of home runs during his career.</td>
</tr>
<tr class="even">
<td align="left"><code>CRuns</code></td>
<td align="left">Number of runs during his career.</td>
</tr>
<tr class="odd">
<td align="left"><code>CRBI</code></td>
<td align="left">Number of runs batted in during his career.</td>
</tr>
<tr class="even">
<td align="left"><code>CWalks</code></td>
<td align="left">Number of walks during his career.</td>
</tr>
<tr class="odd">
<td align="left"><code>League</code></td>
<td align="left">A factor with levels A and N indicating player’s league at the end of 1986.</td>
</tr>
<tr class="even">
<td align="left"><code>Division</code></td>
<td align="left">A factor with levels E and W indicating player’s division at the end of 1986.</td>
</tr>
<tr class="odd">
<td align="left"><code>PutOuts</code></td>
<td align="left">Number of put outs in 1986.</td>
</tr>
<tr class="even">
<td align="left"><code>Assists</code></td>
<td align="left">Number of assists in 1986.</td>
</tr>
<tr class="odd">
<td align="left"><code>Errors</code></td>
<td align="left">Number of errors in 1986.</td>
</tr>
<tr class="even">
<td align="left"><code>NewLeague</code></td>
<td align="left">A factor with levels A and N indicating player’s league at the beginning of 1987.</td>
</tr>
</tbody>
</table>
</div>
<div id="variable-description-of-college_train-and-college_test" class="section level4">
<h4>Variable description of <code>college_train</code> and <code>college_test</code></h4>
<table>
<colgroup>
<col width="26%" />
<col width="73%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Name</th>
<th align="left">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><code>Private</code></td>
<td align="left">A factor with levels No and Yes indicating private or public university.</td>
</tr>
<tr class="even">
<td align="left"><code>Apps</code></td>
<td align="left">Number of applications received.</td>
</tr>
<tr class="odd">
<td align="left"><code>Accept</code></td>
<td align="left">Number of applications accepted.</td>
</tr>
<tr class="even">
<td align="left"><code>Enroll</code></td>
<td align="left">Number of new students enrolled.</td>
</tr>
<tr class="odd">
<td align="left"><code>Top10perc</code></td>
<td align="left">Pct. new students from top 10% of H.S. class.</td>
</tr>
<tr class="even">
<td align="left"><code>Top25perc</code></td>
<td align="left">Pct. new students from top 25% of H.S. class.</td>
</tr>
<tr class="odd">
<td align="left"><code>F.Undergrad</code></td>
<td align="left">Number of fulltime undergraduates.</td>
</tr>
<tr class="even">
<td align="left"><code>P.Undergrad</code></td>
<td align="left">Number of parttime undergraduates.</td>
</tr>
<tr class="odd">
<td align="left"><code>Outstate</code></td>
<td align="left">Out-of-state tuition.</td>
</tr>
<tr class="even">
<td align="left"><code>Room.Board</code></td>
<td align="left">Room and board costs.</td>
</tr>
<tr class="odd">
<td align="left"><code>Books</code></td>
<td align="left">Estimated book costs.</td>
</tr>
<tr class="even">
<td align="left"><code>Personal</code></td>
<td align="left">Estimated personal spending.</td>
</tr>
<tr class="odd">
<td align="left"><code>PhD</code></td>
<td align="left">Pct. of faculty with Ph.D.’s.</td>
</tr>
<tr class="even">
<td align="left"><code>Terminal</code></td>
<td align="left">Pct. of faculty with terminal degree.</td>
</tr>
<tr class="odd">
<td align="left"><code>S.F.Ratio</code></td>
<td align="left">Student/faculty ratio.</td>
</tr>
<tr class="even">
<td align="left"><code>perc.alumni</code></td>
<td align="left">Pct. alumni who donate.</td>
</tr>
<tr class="odd">
<td align="left"><code>Expend</code></td>
<td align="left">Instructional expenditure per student.</td>
</tr>
<tr class="even">
<td align="left"><code>Grad.Rate</code></td>
<td align="left">Graduation rate.</td>
</tr>
</tbody>
</table>
</div>
<div id="variable-description-of-house_train-and-house_test" class="section level4">
<h4>Variable description of <code>house_train</code> and <code>house_test</code></h4>
<table>
<colgroup>
<col width="26%" />
<col width="73%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Name</th>
<th align="left">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><code>price</code></td>
<td align="left">Price of the house in $.</td>
</tr>
<tr class="even">
<td align="left"><code>bedrooms</code></td>
<td align="left">Number of bedrooms.</td>
</tr>
<tr class="odd">
<td align="left"><code>bathrooms</code></td>
<td align="left">Number of bathrooms.</td>
</tr>
<tr class="even">
<td align="left"><code>sqft_living</code></td>
<td align="left">Square footage of the home.</td>
</tr>
<tr class="odd">
<td align="left"><code>sqft_lot</code></td>
<td align="left">Square footage of the lot.</td>
</tr>
<tr class="even">
<td align="left"><code>floors</code></td>
<td align="left">Total floors (levels) in house.</td>
</tr>
<tr class="odd">
<td align="left"><code>waterfront</code></td>
<td align="left">House which has a view to a waterfront.</td>
</tr>
<tr class="even">
<td align="left"><code>view</code></td>
<td align="left">Has been viewed.</td>
</tr>
<tr class="odd">
<td align="left"><code>condition</code></td>
<td align="left">How good the condition is (Overall).</td>
</tr>
<tr class="even">
<td align="left"><code>grade</code></td>
<td align="left">Overall grade given to the housing unit, based on King County grading system.</td>
</tr>
<tr class="odd">
<td align="left"><code>sqft_above</code></td>
<td align="left">Square footage of house apart from basement.</td>
</tr>
<tr class="even">
<td align="left"><code>sqft_basement</code></td>
<td align="left">Square footage of the basement.</td>
</tr>
<tr class="odd">
<td align="left"><code>yr_built</code></td>
<td align="left">Built Year.</td>
</tr>
<tr class="even">
<td align="left"><code>yr_renovated</code></td>
<td align="left">Year when house was renovated.</td>
</tr>
<tr class="odd">
<td align="left"><code>zipcode</code></td>
<td align="left">Zip code.</td>
</tr>
<tr class="even">
<td align="left"><code>lat</code></td>
<td align="left">Latitude coordinate.</td>
</tr>
<tr class="odd">
<td align="left"><code>long</code></td>
<td align="left">Longitude coordinate.</td>
</tr>
<tr class="even">
<td align="left"><code>sqft_living15</code></td>
<td align="left">Living room area in 2015 (implies some renovations). This might or might not have affected the lotsize area.</td>
</tr>
<tr class="odd">
<td align="left"><code>sqft_lot15</code></td>
<td align="left">lot-size area in 2015 (implies some renovations).</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="functions" class="section level2">
<h2>Functions</h2>
<div id="packages" class="section level3">
<h3>Packages</h3>
<table>
<thead>
<tr class="header">
<th align="left">Package</th>
<th align="left">Installation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><code>tidyverse</code></td>
<td align="left"><code>install.packages(&quot;tidyverse&quot;)</code></td>
</tr>
<tr class="even">
<td align="left"><code>caret</code></td>
<td align="left"><code>install.packages(&quot;caret&quot;)</code></td>
</tr>
<tr class="odd">
<td align="left"><code>partykit</code></td>
<td align="left"><code>install.packages(&quot;partykit&quot;)</code></td>
</tr>
<tr class="even">
<td align="left"><code>party</code></td>
<td align="left"><code>install.packages(&quot;party&quot;)</code></td>
</tr>
</tbody>
</table>
</div>
<div id="functions-1" class="section level3">
<h3>Functions</h3>
<table>
<colgroup>
<col width="7%" />
<col width="12%" />
<col width="80%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Function</th>
<th align="left">Package</th>
<th align="left">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><code>trainControl()</code></td>
<td align="left"><code>caret</code></td>
<td align="left">Define modelling control parameters</td>
</tr>
<tr class="even">
<td align="left"><code>train()</code></td>
<td align="left"><code>caret</code></td>
<td align="left">Train a model</td>
</tr>
<tr class="odd">
<td align="left"><code>predict(object, newdata)</code></td>
<td align="left"><code>stats</code></td>
<td align="left">Predict the criterion values of <code>newdata</code> based on <code>object</code></td>
</tr>
<tr class="even">
<td align="left"><code>postResample()</code></td>
<td align="left"><code>caret</code></td>
<td align="left">Calculate aggregate model performance in regression tasks</td>
</tr>
<tr class="odd">
<td align="left"><code>confusionMatrix()</code></td>
<td align="left"><code>caret</code></td>
<td align="left">Calculate aggregate model performance in classification tasks</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="resources" class="section level2">
<h2>Resources</h2>
<figure>
<center>
<a href="https://github.com/rstudio/cheatsheets/raw/master/caret.pdf"> <img src="https://www.rstudio.com/wp-content/uploads/2015/01/caret-cheatsheet.png" alt="Trulli" style="width:70%"></a><br> <font style="font-size:10px"> from <a href= "https://github.com/rstudio/cheatsheets/raw/master/caret.pdf</figcaption">github.com/rstudio</a></font>
</figure>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
